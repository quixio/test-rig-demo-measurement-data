# Quix Project Descriptor
# This file describes the data pipeline and configuration of resources of a Quix Project.

metadata:
  version: 1.0

# This section describes the Deployments of the data pipeline
deployments:
  - name: test-rig-data
    application: test-rig-data
    version: latest
    deploymentType: Service
    resources:
      cpu: 200
      memory: 500
      replicas: 1
    publicAccess:
      enabled: true
      urlPrefix: test-rig-data
    variables:
      - name: output
        inputType: OutputTopic
        description: This is the output topic for hello world data
        required: true
        value: edf-data
  - name: Config Enricher
    application: config-enricher
    version: latest
    deploymentType: Service
    resources:
      cpu: 200
      memory: 500
      replicas: 1
    variables:
      - name: DATA_TOPIC
        inputType: InputTopic
        description: Name of the ingress data topic.
        value: normalised-data
      - name: CONFIG_TOPIC
        inputType: InputTopic
        description: Name of the ingress config topic.
        value: config-updates
      - name: OUTPUT_TOPIC
        inputType: OutputTopic
        description: Name of the output topic to write enriched data to.
        value: config-enriched-data
      - name: CONSUMER_GROUP
        inputType: FreeText
        description: Consumer group name.
        required: true
        value: config-enricher
      - name: CONFIG_SDK_TOKEN
        inputType: Secret
        description: SDK token from the config manager's project
        required: true
        secretKey: config_sdk_token
  - name: Quix TS Datalake Sink
    image: quixcontainerregistry.azurecr.io/quix-datalake-ts-sink:20250917.1-auth
    deploymentType: Service
    resources:
      cpu: 200
      memory: 500
      replicas: 1
    variables:
      - name: input
        inputType: InputTopic
        description: Name of the Kafka input topic to consume from
        required: true
        value: config-enriched-data
      - name: S3_BUCKET
        inputType: FreeText
        description: S3 bucket name for storing Parquet files
        required: true
        value: quixdatalaketest
      - name: S3_PREFIX
        inputType: FreeText
        description: S3 prefix/path for data files
        value: ts_test
      - name: TABLE_NAME
        inputType: FreeText
        description: Table name for data organization and registration
        value: test_runs
      - name: HIVE_COLUMNS
        inputType: FreeText
        description: Comma-separated list of columns for Hive partitioning. Include year/month/day/hour to extract from TIMESTAMP_COLUMN (e.g., location,year,month,day,sensor_type)
        value: campaign_id,environment_id,test_id
      - name: TIMESTAMP_COLUMN
        inputType: FreeText
        description: Column containing timestamp values to extract year/month/day/hour from
        value: kafka_timestamp
      - name: CATALOG_URL
        inputType: FreeText
        description: REST Catalog URL for optional table registration (leave empty to skip)
        value: https://iceberg-catalog-quixers-testmanagerdemo-dev.az-france-0.app.quix.io
      - name: CATALOG_AUTH_TOKEN
        inputType: Secret
        description: auth token for catalog
        secretKey: quixlake_sdk_token
      - name: AUTO_DISCOVER
        inputType: FreeText
        description: Automatically register table in REST Catalog on first write
        value: true
      - name: CATALOG_NAMESPACE
        inputType: FreeText
        description: Catalog namespace for table registration
        value: default
      - name: BATCH_SIZE
        inputType: FreeText
        description: Number of messages to batch before writing to S3
        value: 1000
      - name: COMMIT_INTERVAL
        inputType: FreeText
        description: Kafka commit interval in seconds
        value: 30
      - name: CONSUMER_GROUP
        inputType: FreeText
        description: Kafka consumer group name
        value: aws_sink
      - name: AUTO_OFFSET_RESET
        inputType: FreeText
        description: Where to start consuming if no offset exists
        value: earliest
      - name: AWS_ACCESS_KEY_ID
        inputType: Secret
        description: AWS Access Key ID for S3 access
        secretKey: aws_access_key
      - name: AWS_SECRET_ACCESS_KEY
        inputType: Secret
        description: AWS Secret Access Key for S3 access
        secretKey: aws_secret_key
      - name: AWS_REGION
        inputType: FreeText
        description: AWS region for S3 bucket
        value: eu-west-2
      - name: LOGLEVEL
        inputType: FreeText
        description: loglevel
        value: INFO
  - name: Quix DataLake Sink
    application: DataLake.Sink
    version: latest
    deploymentType: Managed
    resources:
      cpu: 500
      memory: 1500
      replicas: 1
    configuration:
      topic: edf-data
      autoOffsetReset: earliest
  - name: Data normalisation
    application: data-normalisation
    version: latest
    deploymentType: Service
    resources:
      cpu: 200
      memory: 500
      replicas: 1
    variables:
      - name: input
        inputType: InputTopic
        description: Name of the input topic to listen to.
        value: edf-data
      - name: output
        inputType: OutputTopic
        description: Name of the output topic to write to.
        value: normalised-data
  - name: Quix DataLake Sink 2
    application: DataLake.Sink
    version: latest
    deploymentType: Managed
    resources:
      cpu: 500
      memory: 1500
      replicas: 1
    configuration:
      topic: config-enriched-data

# This section describes the Topics of the data pipeline
topics:
  - name: edf-data
  - name: config-enriched-data
  - name: normalised-data
  - name: config-updates
